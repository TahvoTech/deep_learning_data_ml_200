{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc04cc8",
   "metadata": {},
   "source": [
    "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563db0a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d41bde4c6dd400f843cf0b91919295f",
     "grade": false,
     "grade_id": "cell-910d80dbd222bcaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "## Task 2: Hyperparameter Tuning (5 Points)\n",
    "\n",
    "In this task, you will enhance your model’s performance through targeted modifications to the architecture. The main objectives include optimizing performance on the training dataset. The goal is to implement various changes and observe how they affect the overall performance of the model in terms of training and validation results, the smoothness and stability of the training curves, the number of trainable parameters, and the training time.\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "\n",
    "**Task 2.1: Increase Convolution Channels** (1 point)\n",
    "\n",
    "    Goal: Modify the model to increase the number of convolution channels to 128 across three layers.\n",
    "\n",
    "**Task 2.2: Add One Convolution Layer** (1 point)\n",
    "\n",
    "    Goal: Add an additional convolution layer to the model, creating four layers with 32 intermediate channels.\n",
    "\n",
    "**Task 2.3: Adjust Kernel Sizes (Smaller)** (1 point)\n",
    "\n",
    "    Goal: Modify the kernel sizes to be smaller than those used in the base model.\n",
    "\n",
    "**Task 2.4: Adjust Kernel Sizes (Larger)** (1 point)\n",
    "\n",
    "    Goal: Modify the kernel sizes to be larger than those used in the base model.\n",
    "\n",
    "**Task 2.5: Change Non-linearities (ReLU)** (1 point)\n",
    "\n",
    "    Goal: Replace the current activation functions with ReLU.\n",
    "\n",
    "### Deliverables from this task:\n",
    "\n",
    "* ex3_02_hyperparameter_tuning.ipynb\n",
    "* 'relu_model.pth'\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8445f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False   # You can set it to True if you want to run inference on your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c688be9-b20b-4996-979e-ee2555da635b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c1ac106c142bbd64dabe53525b11c45",
     "grade": true,
     "grade_id": "cell-16ebf98cca481d32",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6feccd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# Set random seeds for all libraries\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1) \n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f32a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a32226df90694eeab24424883416f261",
     "grade": false,
     "grade_id": "cell-fb7b422b236c2e0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272855a-cca5-4a9e-b96b-84af270fae60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94a18ae73734f2f529dd312711a2e593",
     "grade": false,
     "grade_id": "cell-781c2fcb5766466d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Add the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573e466-40f0-41a8-adcc-7015cb657a41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"dataset_ex3\" # you can change the path if you want to store the dataset somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a919c-72cb-4472-a101-90df507c5a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c76e468a8231134e85d970a6032cf859",
     "grade": true,
     "grade_id": "cell-888628addb87c0ca",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403efef-8583-4c83-bcd8-efe439d40ab3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85faa59e1ac9689ab67019f60507ddc1",
     "grade": false,
     "grade_id": "cell-7429909f377b1aef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_file_list(path, validation_split):\n",
    "    \n",
    "    audio_class_names = ['music', 'speech']\n",
    "    data_directories = {'music': path + '/music_wav', \n",
    "                        'speech': path + '/speech_wav'}\n",
    "    \n",
    "    audio_files = {class_name: [] for class_name in audio_class_names}\n",
    "    for class_name in audio_class_names:\n",
    "        folder = data_directories[class_name]\n",
    "        filelist = os.listdir(folder)\n",
    "        for filename in filelist:\n",
    "            if filename.endswith('.wav'):\n",
    "                audio_files[class_name].append(os.path.join(folder, filename))\n",
    "       \n",
    "    np.random.seed(1)\n",
    "    dataset_split = {'train': [], 'val': []}\n",
    "    for class_id, class_name in enumerate(audio_class_names):\n",
    "        n_data = len(audio_files[class_name])\n",
    "        random_indices = np.random.permutation(n_data)\n",
    "        n_validation = int(validation_split * n_data)\n",
    "        val_indices = random_indices[:n_validation]\n",
    "        train_indices = random_indices[n_validation:]\n",
    "        dataset_split['train'] += [(audio_files[class_name][k], class_id) for k in train_indices] \n",
    "        dataset_split['val'] += [(audio_files[class_name][k], class_id) for k in val_indices] \n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ec9a3-ae6d-46ee-952b-4ebfe9f7672b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "722635170df78438e061235185a70864",
     "grade": false,
     "grade_id": "cell-1546acec0982c4ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MSDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, filelist, sample_sec=5., is_train=True):\n",
    "        self.filelist = filelist\n",
    "        self.time_duration = sample_sec\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        _, sf = librosa.load(filelist[0][0], sr = None)\n",
    "        self.sf = sf\n",
    "        self.n_features = int(self.time_duration * sf)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        audio_file, class_id = self.filelist[i]\n",
    "        x, sf = librosa.load(audio_file, sr = None)\n",
    "        k = 0\n",
    "            \n",
    "        x = torch.from_numpy(x[k:k+self.n_features]).reshape(1,-1)\n",
    "        \n",
    "        return x, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123f574-62c2-48ef-af6e-66dac5187107",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04d0e1f10274a60c8b3e9c31ab92b79f",
     "grade": false,
     "grade_id": "cell-bd6b8ef63cff57f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Training and validation loops: \n",
    "\n",
    "Fill in the blanks as instructed in the code.\n",
    "\n",
    "**Hint:** Replace the next cell with the training and validation loops from your solution to Task 1 in the ex3_01_base_model.ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b731df9-5f30-4168-a749-fe8607ad3856",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87930d357b022c8dac2ca500baad0524",
     "grade": false,
     "grade_id": "cell-489f20e61072497e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optim, model, loss_fn, dl_train, dl_val, hist=None):\n",
    "    np.random.seed(1)\n",
    "    if hist is not None:\n",
    "        pass\n",
    "    else:\n",
    "        hist = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    t_initial = time.time()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        start = time.time()\n",
    "        train_loss, train_accuracy = 0., 0.\n",
    "        num_samples = 0\n",
    "        \n",
    "        for input_batch, target_batch in dl_train: \n",
    "            # your code here for minibatch training\n",
    "            # 1. call batch data and labels and set them to the correct device\n",
    "            # 2. make the prediction on the data\n",
    "            # 3. calculate loss\n",
    "            # 4. set optimizer to zero grad\n",
    "            # 5. do backward pass\n",
    "            # 6. move the optimizer one step forward\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # accumulate correct prediction\n",
    "            train_accuracy += (torch.argmax(predictions.detach(), dim=1) == target_batch).sum().item() # number of correct predictions\n",
    "            train_loss += loss_train.item() * input_batch.shape[0]\n",
    "            num_samples += input_batch.shape[0]\n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        train_accuracy /= num_samples       \n",
    "        val_loss, val_accuracy = validation_loop(model, loss_fn, dl_val)\n",
    "        \n",
    "        end = time.time()\n",
    "        epoch_time = round(end - start, 2)\n",
    "        if epoch <= 5 or epoch % 10 == 0 or epoch == n_epochs:\n",
    "             print(f'Epoch {epoch}, train_loss {train_loss:.2f}, train_accuracy: {train_accuracy:.4f}, '\n",
    "                   f'val_loss {val_loss:.2f}, val_accuracy: {val_accuracy:.4f}, time = {epoch_time}')\n",
    "\n",
    "        # record for history return\n",
    "        hist['train_loss'].append(train_loss)\n",
    "        hist['val_loss'].append(val_loss) \n",
    "        hist['train_accuracy'].append(train_accuracy)\n",
    "        hist['val_accuracy'].append(val_accuracy)\n",
    "        \n",
    "    t_final = time.time()\n",
    "    t_total = round(t_final - t_initial, 2)\n",
    "    minutes = int(t_total // 60)\n",
    "    seconds = int(t_total % 60)\n",
    "    print(f'Finished training_loop() within {minutes} minutes and {seconds} seconds')\n",
    "    return hist\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_accuracy, num_samples = 0., 0., 0.\n",
    "        \n",
    "        for input_batch, target_batch in dataloader:\n",
    "            # your code here for minibatch validation\n",
    "            # 1. set input_batch, target_batch to correct device\n",
    "            # 2. make the prediction on input_batch\n",
    "            # 3. calculate loss and add it to previous loss\n",
    "            # 4. obtain predicted class labels from predictions (hint: use torch.argmax)\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            ###\n",
    "            \n",
    "            total_accuracy += (predicted_classes == target_batch).sum().item()\n",
    "            num_samples += len(target_batch)\n",
    "    \n",
    "    average_loss = total_loss / num_samples\n",
    "    average_accuracy = total_accuracy / num_samples\n",
    "    \n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].plot(history['train_loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'], label='Validation')\n",
    "    axes[0].legend()\n",
    "\n",
    "    max_val_accuracy = max(history['val_accuracy'])\n",
    "    axes[1].set_title(f'Accuracy (Best: {max_val_accuracy:.2f})')\n",
    "    axes[1].plot(history['train_accuracy'], label='Train')\n",
    "    axes[1].plot(history['val_accuracy'], label='Validation')\n",
    "    axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60641618-7e4d-415c-a22c-299fa42a13de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6380d8ec1657cbd9b8731792550f130",
     "grade": false,
     "grade_id": "cell-12a009697bd7d240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 2.1. Increase Convolution Channels\n",
    "\n",
    "Use the base model from Task 1, increasing the convolutional channels from 32 to 128. Fill in the blanks in the cell below as instructed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257613f5-299a-43fd-bec1-22b0f5766c67",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7a943977324ca5074ffc441941a1815",
     "grade": false,
     "grade_id": "cell-d0c2abdcc01e2b7d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, nonlin=\"Tanh\"):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    kernel_size=11,\n",
    "                                    stride=5)\n",
    "        if nonlin == \"ELU\":\n",
    "            self.activation_fn = nn.ELU()\n",
    "        elif nonlin == \"ReLU\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif nonlin == \"Tanh\":\n",
    "            self.activation_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 128\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Input channels: 128 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 5. Create a flattening layer to flatten the output for the final layer\n",
    "        # 6. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Apply global average pooling to reduce dimensions\n",
    "        # 5. Flatten the pooled output\n",
    "        # 6. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c6945-5ca4-4729-88df-1da10f700864",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d832433956e3de250e2dfd4bcf1e73b7",
     "grade": false,
     "grade_id": "cell-c0594868fee75ccf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2dfab-c68a-4b19-8a2b-d1454ed47e9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe0957fcdc9e3b47847759959f243f84",
     "grade": true,
     "grade_id": "cell-e4ade0023d349447",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"Tanh\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 3:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [128, 128, 2]  # Expected output channels for the three layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        \n",
    "# Check the output shape\n",
    "expected_shape = (1, 2)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 184706\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    \n",
    "# Check the output range for LogSoftmax (should be <= 0)\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e731922-554a-4580-84eb-8784bfbd0e6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b34fbf87ffa701d1643b69401e7b0544",
     "grade": false,
     "grade_id": "cell-7e430867659bfb48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Now, run the cell below to apply training and validation. \n",
    "\n",
    "Compare the results with those from the base model in Task 1. Observe how increasing the number of channels in the convolutional layers affects the model's performance on the training and validation splits. Additionally, note how this change impacts the number of training parameters and the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9aced4-bdfa-452d-82a6-3dad51e058b7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    plot_history(history)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8db971-da19-470f-9858-02e022a8964a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea08143ae9ee5e702ce41bd96f59009d",
     "grade": true,
     "grade_id": "cell-42c15a8554934d62",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests for checking the performance of the trained model\n",
    "all_tests_successful = True\n",
    "if not skip_training:\n",
    "    try:\n",
    "    \n",
    "        # Test 1: Ensure training accuracy is within the correct range\n",
    "        max_tacc = max(history['train_accuracy'])\n",
    "        if not (0.5 <= max_tacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n",
    "            \n",
    "        # Test 2: Ensure accuracy is within the correct range\n",
    "        max_vacc = max(history['val_accuracy'])\n",
    "        if not (0.5 <= max_vacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n",
    "    \n",
    "        if all_tests_successful:\n",
    "            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n",
    "    \n",
    "    except AssertionError as e:\n",
    "        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28257f16-c157-493b-a38e-05dc178a7363",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "905c7b6f33cadc0cb6b11f55bab8733a",
     "grade": false,
     "grade_id": "cell-9f66b3822bb42a48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 2.2: Add One Convolution Layer\n",
    "\n",
    "Next, use the base model from Task 1 (32 channels), adding one extra the convolutional layers to have 4 convolutional layers with 32 intermediate filters (channels). Fill in the blanks in the cell below as instructed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256000b8",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64007428b144699835682d47ed09c330",
     "grade": false,
     "grade_id": "cell-548b583f142f383c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, nonlin=\"Tanh\"):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    kernel_size=11,\n",
    "                                    stride=5)\n",
    "        if nonlin == \"ELU\":\n",
    "            self.activation_fn = nn.ELU()\n",
    "        elif nonlin == \"ReLU\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif nonlin == \"Tanh\":\n",
    "            self.activation_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 32\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create the forth hidden layer using BasicBlock\n",
    "        #    - Input channels: 32 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 5. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 6. Create a flattening layer to flatten the output for the final layer\n",
    "        # 7. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Pass the output to the forth hidden layer\n",
    "        # 5. Apply global average pooling to reduce dimensions\n",
    "        # 6. Flatten the pooled output\n",
    "        # 7. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95911b39-5b79-4e7d-9182-050b23b55822",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fa49102a197bd2c265f21609cd78402",
     "grade": false,
     "grade_id": "cell-b790c4ae3118b732",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bd08d-4b69-458f-8934-705f3de57924",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d0d2c6e43b14bdb3a271642b123c413",
     "grade": true,
     "grade_id": "cell-7a4a4efaf07be6e1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"Tanh\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 4:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [32, 32, 32, 2]  # Expected output channels for the three layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        \n",
    "# Check the output shape\n",
    "expected_shape = (1, 2)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 23682\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    \n",
    "# Check the output range for LogSoftmax (should be <= 0)\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8c4877-6187-4caf-bd76-7f4fe298a02b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d18edf46062d2496e69b5976bfd8f075",
     "grade": false,
     "grade_id": "cell-b93cb115cdfee2d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Now, run the cell below to apply training and validation. \n",
    "\n",
    "Compare the results with those from the base model in Task 1 and the model from Task 2.1. Observe how increasing the number of convolutional layers affects the model's performance on the training and validation splits. Additionally, note how this change impacts the number of training parameters and the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c4b20-e09d-4b7b-868e-6fd6db12ddcb",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4def87-6069-462a-9529-c657c4738be1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbf50c8874603946af3451db7bc94767",
     "grade": false,
     "grade_id": "cell-7d8f9956b4d654e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 2.3: Adjust Kernel Sizes (Smaller)\n",
    "\n",
    "In this task, you will modify the base model by using smaller kernel sizes for the convolutional layers. Set the kernel size to 7 and the stride to 3. Fill in the blanks in the code cell below as directed to apply these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b1785-164e-4d68-ab3e-f1c72e15e20f",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cd15700cc2caf001c2e4adf2f66713b",
     "grade": false,
     "grade_id": "cell-d12415a66ffdb425",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, nonlin=\"Tanh\"):\n",
    "        super().__init__()\n",
    "        # Define the convolutional layer with a kernel size of 7 and stride of 3\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        if nonlin == \"ELU\":\n",
    "            self.activation_fn = nn.ELU()\n",
    "        elif nonlin == \"ReLU\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif nonlin == \"Tanh\":\n",
    "            self.activation_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "        \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 32\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Input channels: 32 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 5. Create a flattening layer to flatten the output for the final layer\n",
    "        # 6. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Apply global average pooling to reduce dimensions\n",
    "        # 5. Flatten the pooled output\n",
    "        # 6. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6d03b7-f5dd-4733-8a14-474a688cd895",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e208b7cfe1ee411b7344cf77a591dae7",
     "grade": false,
     "grade_id": "cell-12f4cab96fd1cad0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db4b971-b2de-429f-9c28-2248061feb73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f9df422a3bfc892eddee53cb155638c",
     "grade": true,
     "grade_id": "cell-bc7bb312fa8ce10e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"Tanh\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 3:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [32, 32, 2]  # Expected output channels for the three layers\n",
    "expected_kernels = [7, 7, 7]\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "    # Check the kernel size\n",
    "    if layer.kernel_size[0] != expected_kernels[i]:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")    \n",
    "# Check the output shape\n",
    "expected_shape = (1, 2)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 7906\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    \n",
    "# Check the output range for LogSoftmax (should be <= 0)\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d31ac-aaa9-41e3-818d-4be21006db78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c037800b5610106b3d15209942700162",
     "grade": false,
     "grade_id": "cell-bcb42689fd6a55af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Now, run the cell below to train and validate the model.\n",
    "\n",
    "Compare the results with those of the base model from Task 1. Observe how using smaller kernel sizes influences the model's performance on the training and validation datasets. Additionally, take note of any changes in the number of training parameters and the training duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf7698-aa3a-443b-8e4b-bd9ac5ee04fd",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce06975-ce25-4532-b8d5-a02cf9fe845f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32f73344aaf65fa9ee7b52b84b6b4a40",
     "grade": false,
     "grade_id": "cell-99535681e4e77e0b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 2.4: Adjust Kernel Sizes (Larger)\n",
    "\n",
    "In this step, return to the base model and modify the kernel sizes of the convolutional layers to be larger. Use kernels of size 22 and a stride of 11. Complete the cell below by filling in the blanks as specified in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4ca076",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3abff25080ad67cf961421040594cded",
     "grade": false,
     "grade_id": "cell-e11477304666c1ea",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, nonlin=\"Tanh\"):\n",
    "        super().__init__()\n",
    "        # Define the convolutional layer with a kernel size of 22 and stride of 11\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        if nonlin == \"ELU\":\n",
    "            self.activation_fn = nn.ELU()\n",
    "        elif nonlin == \"ReLU\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif nonlin == \"Tanh\":\n",
    "            self.activation_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "        \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 32\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Input channels: 32 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 5. Create a flattening layer to flatten the output for the final layer\n",
    "        # 6. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Apply global average pooling to reduce dimensions\n",
    "        # 5. Flatten the pooled output\n",
    "        # 6. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb080fb2-0d1a-4879-a1bc-1bc7c6571708",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ed64110f0f362389a031359085badec",
     "grade": false,
     "grade_id": "cell-c678d8ccbea1b779",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578418b-75aa-4be3-af87-b616908cdbcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4543d8fe1100a74512b337e296698add",
     "grade": true,
     "grade_id": "cell-f411dd0ae3bb1875",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"Tanh\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "expected_kernels = [22, 22, 22]\n",
    "\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 3:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [32, 32, 2]  # Expected output channels for the three layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "    # Check the kernel size\n",
    "    if layer.kernel_size[0] != expected_kernels[i]:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")    \n",
    "\n",
    "# Check the output shape\n",
    "expected_shape = (1, 2)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 24706\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    \n",
    "# Check the output range for LogSoftmax (should be <= 0)\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea246ba-9a77-440b-989d-2e125a6d5267",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e08d8152edfe88232bcdcc79b5eabab4",
     "grade": false,
     "grade_id": "cell-9d0cd2d4034d0dfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Run the cell below to apply training and validation.\n",
    "\n",
    "Compare these results with those from the base model in Task 1 and the smaller kernel sizes in Task 2.3. Observe how using larger kernel sizes influences the model's performance on the training and validation sets. Also, note any changes in the number of training parameters and training time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ed437-7c3c-433d-8c95-d5d679d17941",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ca55c-f547-4084-9335-e8f78bd9b308",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "51a697b621bb3bb8438c9075c10a56bc",
     "grade": false,
     "grade_id": "cell-a899cf1c6d488e9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 2.5: Change Non-linearities\n",
    "\n",
    "For this task, return to the base model and explore the impact of different non-linear activation functions on the model's performance.\n",
    "\n",
    "Re-create the base model by filling in the blanks in the cell below as instructed. Try using \"ReLU\" as an alternative activation function to see how that affect training and validation results.\n",
    "\n",
    "Save the model as **'relu_model.pth'** and submit it to Moodle along with your other files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd955b-6a63-4357-915c-655fc7d000e4",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc82ef94f730e8c28a18b8dd07fac2d3",
     "grade": false,
     "grade_id": "cell-da7835087865b696",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, nonlin=\"Tanh\"):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    kernel_size=11,\n",
    "                                    stride=5)\n",
    "        if nonlin == \"ELU\":\n",
    "            self.activation_fn = nn.ELU()\n",
    "        elif nonlin == \"ReLU\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif nonlin == \"Tanh\":\n",
    "            self.activation_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "        \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 32\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Input channels: 32 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 5. Create a flattening layer to flatten the output for the final layer\n",
    "        # 6. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Apply global average pooling to reduce dimensions\n",
    "        # 5. Flatten the pooled output\n",
    "        # 6. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ea4a77-03e3-488f-ae30-f3cbb4c3cc75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "891127791b75b1d9d178381ab7b8893f",
     "grade": false,
     "grade_id": "cell-e8a3b86f8689161e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "In this task, set the non-linearity function in the convolutional layers to **\"ReLU\"**. Fill in the blanks in the cell below following the instructions in the code. \n",
    "\n",
    "After training, compare the results with the base model that used **\"Tanh\"** as the activation function. Observe any differences in training and validation performance, noting any changes in training dynamics, convergence rate, or accuracy.\n",
    "\n",
    "Remember to save the model as **'relu_model.pth'** and submit it to Moodle along with your other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ba9d4-b82a-449a-a826-da157065ff6c",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06d7de584a672ee6646fc3a5a742f85b",
     "grade": false,
     "grade_id": "cell-01b6645df00eba16",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "# your code here for calling model with \"ReLU\" non-linearity\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'relu_model.pth')\n",
    "    plot_history(history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c5289-ad0a-4355-981b-b63f0f3bc2ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43190f4fbf0cd1dba9b335cef67b563e",
     "grade": true,
     "grade_id": "cell-e77eb518e15f1d74",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests for checking the performance of the trained model\n",
    "all_tests_successful = True\n",
    "if not skip_training:\n",
    "    try:\n",
    "        # Test 0: Ensure non-linearity is ReLU\n",
    "        basic_blocks = [layer for layer in model.modules() if isinstance(layer, BasicBlock)]\n",
    "        for i, block in enumerate(basic_blocks):\n",
    "            if not isinstance(block.activation_fn, nn.ReLU):\n",
    "                all_tests_successful = False\n",
    "                raise AssertionError(f\"BasicBlock {i + 1} does not use ReLU as the activation function.\")\n",
    "    \n",
    "        # Test 1: Ensure training accuracy is within the correct range\n",
    "        max_tacc = max(history['train_accuracy'])\n",
    "        if not (0.5 <= max_tacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n",
    "            \n",
    "        # Test 2: Ensure accuracy is within the correct range\n",
    "        max_vacc = max(history['val_accuracy'])\n",
    "        if not (0.5 <= max_vacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n",
    "    \n",
    "        if all_tests_successful:\n",
    "            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n",
    "    \n",
    "    except AssertionError as e:\n",
    "        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2266c6-86ca-448e-9c20-57aa26bf182a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "381afb4afd08d43349a432761d61d384",
     "grade": true,
     "grade_id": "cell-3b433ad8ad2b2d56",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e12a1-eff5-4f26-be4e-a10a19edc378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
