{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99abe309",
   "metadata": {},
   "source": [
    "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563db0a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feadf60beeed814455ff25ed310c1c00",
     "grade": false,
     "grade_id": "cell-910d80dbd222bcaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "## Task 4: Regularization Practices (5 Points)\n",
    "\n",
    "In Task 2 you have tried differnt hyperparameter tuning techniques to increase the performance of the model on the training set. In this task, you will practice regularization techniques to help the model to generalize to unseen data, i.e., to increase the performance on the validation set. You are asked to start with a model that achieves a good performance on the training set compared to the base model. However, the number of trainable parameters in this model is large which leads to over-fitting as observed in the training and validation curves. Here we try different techniques to decreadse the number of trainable påarameters and to increase the model performance on validation set. \n",
    "\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "\n",
    "**Task 4.1: Experiment with normalization layers** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of batch normalization in model generalization and training stability.\n",
    "\n",
    "**Task 4.2: Experiment with dropout layers** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of dropout layers in model generalization.\n",
    "\n",
    "**Task 4.3: Experiment with efficient model architecture through pooling layers** (3 point)\n",
    "\n",
    "    Goal: Observe the effect of efficient model design through adjusting receptive field of layers.\n",
    "\n",
    "\n",
    "### Deliverables from this task:\n",
    "\n",
    "* ex3_04_regularization.ipynb\n",
    "* 'normalized_model.pth'\n",
    "* 'pooled_model.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8445f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False   # You can set it to True if you want to run inference on your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c688be9-b20b-4996-979e-ee2555da635b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c1ac106c142bbd64dabe53525b11c45",
     "grade": true,
     "grade_id": "cell-16ebf98cca481d32",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6feccd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# Set random seeds for all libraries\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1) \n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f32a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b32a19faebd8b693e7f1657cf01c0ba5",
     "grade": false,
     "grade_id": "cell-7c82bfc95c4228d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272855a-cca5-4a9e-b96b-84af270fae60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94a18ae73734f2f529dd312711a2e593",
     "grade": false,
     "grade_id": "cell-781c2fcb5766466d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Add the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573e466-40f0-41a8-adcc-7015cb657a41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"dataset_ex3\" # you can change the path if you want to store the dataset somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a919c-72cb-4472-a101-90df507c5a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c76e468a8231134e85d970a6032cf859",
     "grade": true,
     "grade_id": "cell-888628addb87c0ca",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403efef-8583-4c83-bcd8-efe439d40ab3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85faa59e1ac9689ab67019f60507ddc1",
     "grade": false,
     "grade_id": "cell-7429909f377b1aef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_file_list(path, validation_split):\n",
    "    \n",
    "    audio_class_names = ['music', 'speech']\n",
    "    data_directories = {'music': path + '/music_wav', \n",
    "                        'speech': path + '/speech_wav'}\n",
    "    \n",
    "    audio_files = {class_name: [] for class_name in audio_class_names}\n",
    "    for class_name in audio_class_names:\n",
    "        folder = data_directories[class_name]\n",
    "        filelist = os.listdir(folder)\n",
    "        for filename in filelist:\n",
    "            if filename.endswith('.wav'):\n",
    "                audio_files[class_name].append(os.path.join(folder, filename))\n",
    "       \n",
    "    np.random.seed(1)\n",
    "    dataset_split = {'train': [], 'val': []}\n",
    "    for class_id, class_name in enumerate(audio_class_names):\n",
    "        n_data = len(audio_files[class_name])\n",
    "        random_indices = np.random.permutation(n_data)\n",
    "        n_validation = int(validation_split * n_data)\n",
    "        val_indices = random_indices[:n_validation]\n",
    "        train_indices = random_indices[n_validation:]\n",
    "        dataset_split['train'] += [(audio_files[class_name][k], class_id) for k in train_indices] \n",
    "        dataset_split['val'] += [(audio_files[class_name][k], class_id) for k in val_indices] \n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ec9a3-ae6d-46ee-952b-4ebfe9f7672b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "722635170df78438e061235185a70864",
     "grade": false,
     "grade_id": "cell-1546acec0982c4ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MSDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, filelist, sample_sec=5., is_train=True):\n",
    "        self.filelist = filelist\n",
    "        self.time_duration = sample_sec\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        _, sf = librosa.load(filelist[0][0], sr = None)\n",
    "        self.sf = sf\n",
    "        self.n_features = int(self.time_duration * sf)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        audio_file, class_id = self.filelist[i]\n",
    "        x, sf = librosa.load(audio_file, sr = None)\n",
    "        k = 0\n",
    "            \n",
    "        x = torch.from_numpy(x[k:k+self.n_features]).reshape(1,-1)\n",
    "        \n",
    "        return x, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123f574-62c2-48ef-af6e-66dac5187107",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04d0e1f10274a60c8b3e9c31ab92b79f",
     "grade": false,
     "grade_id": "cell-bd6b8ef63cff57f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Training and validation loops: \n",
    "\n",
    "Fill in the blanks as instructed in the code.\n",
    "\n",
    "**Hint:** Replace the next cell with the training and validation loops from your solution to Task 1 in the ex3_01_base_model.ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b731df9-5f30-4168-a749-fe8607ad3856",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87930d357b022c8dac2ca500baad0524",
     "grade": false,
     "grade_id": "cell-489f20e61072497e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optim, model, loss_fn, dl_train, dl_val, hist=None):\n",
    "    np.random.seed(1)\n",
    "    if hist is not None:\n",
    "        pass\n",
    "    else:\n",
    "        hist = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    t_initial = time.time()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        start = time.time()\n",
    "        train_loss, train_accuracy = 0., 0.\n",
    "        num_samples = 0\n",
    "        \n",
    "        for input_batch, target_batch in dl_train: \n",
    "            # your code here for minibatch training\n",
    "            # 1. call batch data and labels and set them to the correct device\n",
    "            # 2. make the prediction on the data\n",
    "            # 3. calculate loss\n",
    "            # 4. set optimizer to zero grad\n",
    "            # 5. do backward pass\n",
    "            # 6. move the optimizer one step forward\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # accumulate correct prediction\n",
    "            train_accuracy += (torch.argmax(predictions.detach(), dim=1) == target_batch).sum().item() # number of correct predictions\n",
    "            train_loss += loss_train.item() * input_batch.shape[0]\n",
    "            num_samples += input_batch.shape[0]\n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        train_accuracy /= num_samples       \n",
    "        val_loss, val_accuracy = validation_loop(model, loss_fn, dl_val)\n",
    "        \n",
    "        end = time.time()\n",
    "        epoch_time = round(end - start, 2)\n",
    "        if epoch <= 5 or epoch % 10 == 0 or epoch == n_epochs:\n",
    "             print(f'Epoch {epoch}, train_loss {train_loss:.2f}, train_accuracy: {train_accuracy:.4f}, '\n",
    "                   f'val_loss {val_loss:.2f}, val_accuracy: {val_accuracy:.4f}, time = {epoch_time}')\n",
    "\n",
    "        # record for history return\n",
    "        hist['train_loss'].append(train_loss)\n",
    "        hist['val_loss'].append(val_loss) \n",
    "        hist['train_accuracy'].append(train_accuracy)\n",
    "        hist['val_accuracy'].append(val_accuracy)\n",
    "        \n",
    "    t_final = time.time()\n",
    "    t_total = round(t_final - t_initial, 2)\n",
    "    minutes = int(t_total // 60)\n",
    "    seconds = int(t_total % 60)\n",
    "    print(f'Finished training_loop() within {minutes} minutes and {seconds} seconds')\n",
    "    return hist\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_accuracy, num_samples = 0., 0., 0.\n",
    "        \n",
    "        for input_batch, target_batch in dataloader:\n",
    "            # your code here for minibatch validation\n",
    "            # 1. set input_batch, target_batch to correct device\n",
    "            # 2. make the prediction on input_batch\n",
    "            # 3. calculate loss and add it to previous loss\n",
    "            # 4. obtain predicted class labels from predictions (hint: use torch.argmax)\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            ###\n",
    "            \n",
    "            total_accuracy += (predicted_classes == target_batch).sum().item()\n",
    "            num_samples += len(target_batch)\n",
    "    \n",
    "    average_loss = total_loss / num_samples\n",
    "    average_accuracy = total_accuracy / num_samples\n",
    "    \n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].plot(history['train_loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'], label='Validation')\n",
    "    axes[0].legend()\n",
    "\n",
    "    max_val_accuracy = max(history['val_accuracy'])\n",
    "    axes[1].set_title(f'Accuracy (Best: {max_val_accuracy:.2f})')\n",
    "    axes[1].plot(history['train_accuracy'], label='Train')\n",
    "    axes[1].plot(history['val_accuracy'], label='Validation')\n",
    "    axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13718aa0-f3db-4d9e-9f29-d29b3afaaa1e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3842703481dae7799c307d910d3b0a8",
     "grade": false,
     "grade_id": "cell-6e8be7f6985536a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Model Architecture: \n",
    "\n",
    "Fill in the blanks as instructed in the code to design the model architecture similar to the base model but with four convolutional blocks, where the number of filters (channels) in each convolutional layer is set to 128.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe5693-be7a-4fa6-bad9-0dc1c5158593",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75899b499b6fdc07c06c44071e8f4e4e",
     "grade": false,
     "grade_id": "cell-04f82c5e6202a0a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, nonlin=\"Tanh\"):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    kernel_size=11,\n",
    "                                    stride=5)\n",
    "        if nonlin == \"ELU\":\n",
    "            self.activation_fn = nn.ELU()\n",
    "        elif nonlin == \"ReLU\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif nonlin == \"Tanh\":\n",
    "            self.activation_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 128\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create the forth hidden layer using BasicBlock\n",
    "        #    - Input channels: 128 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 5. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 6. Create a flattening layer to flatten the output for the final layer\n",
    "        # 7. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Pass the output to the forth hidden layer\n",
    "        # 5. Apply global average pooling to reduce dimensions\n",
    "        # 6. Flatten the pooled output\n",
    "        # 7. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5301c5-a1c2-4db8-968b-2ed5fe38320a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1c70b631b289aec1f3034d8a911b5f6",
     "grade": false,
     "grade_id": "cell-32f102c1e9e0f070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your model architecture solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40155de5-7812-41f2-af12-06ac1924bd4c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de031a86ea3239e4909d806a42fb3ea9",
     "grade": true,
     "grade_id": "cell-bd7956d352649e0e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"Tanh\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 4:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [128, 128, 128, 2]  # Expected output channels for the three layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        \n",
    "# Check the output shape\n",
    "expected_shape = (1, 2)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 365058\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    \n",
    "# Check the output range for LogSoftmax (should be <= 0)\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42495d8-8267-422d-8fd3-fe476c97287c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c56e1fb0148a39924305297d5c6993e",
     "grade": false,
     "grade_id": "cell-6cb5ae8334daa814",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Now, run the cell below to train and validate the model with the \"ReLU\" non-linearity, using Adam optimizer with lr = 0.0001.\n",
    "\n",
    "Compare the results with those from the base model in Task 1, and observe how increasing the number of convolutional layers and filters affects the model's performance on the training and validation splits. Additionally, note how this change impacts the number of training parameters and the training time.\n",
    "\n",
    "Repeat the training several times. Do you observe any patterns of instability or overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d64998-0b0c-40c2-b344-ee8c4ce4c74b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"ReLU\").to(device)\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f1330-1151-45de-a3c5-96fee4f70d7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ef6aca5cf506c9854f5602df0855bf5",
     "grade": false,
     "grade_id": "cell-31ba4c3b784c549e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 4.1: Experiment with normalization layers\n",
    "\n",
    "Add a batch normalization layer after the convolution layer and before applying the non-linearity function. Train the model, monitor the behavior of the training and validation curves, and observe how the normalization layer affects the validation performance.\n",
    "\n",
    "Save the model as **'normalized_model.pth'** and submit it to Moodle along with your other files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee7e12",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b07acb2b1584b6f32960fa699526183",
     "grade": false,
     "grade_id": "cell-4aa789d9ff4e0423",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    # Define a convolutional block based on the base model:\n",
    "    # - Add a batch normalization layer after the convolution layer and before the non-linearity function.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 128\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create the forth hidden layer using BasicBlock\n",
    "        #    - Input channels: 128 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 5. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 6. Create a flattening layer to flatten the output for the final layer\n",
    "        # 7. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Pass the output to the forth hidden layer\n",
    "        # 5. Apply global average pooling to reduce dimensions\n",
    "        # 6. Flatten the pooled output\n",
    "        # 7. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c6945-5ca4-4729-88df-1da10f700864",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c43a59bf4d318e99d7ddf1d689cf8c15",
     "grade": false,
     "grade_id": "cell-c0594868fee75ccf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your model architecture solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924050b8-c97b-4181-b037-87e91ab41ae2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbab78cd67a57a74d3d34d6ca417ead7",
     "grade": true,
     "grade_id": "cell-e763e424d1807256",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"ReLU\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "\n",
    "# Dictionary to hold the execution order of each BasicBlock's layers\n",
    "layer_execution_order = {}\n",
    "# Function to capture forward pass order of layers within each BasicBlock\n",
    "def track_execution_order(module, input, output, name):\n",
    "    layer_types = []\n",
    "    for sub_module in module.children():  # Iterate through layers within BasicBlock\n",
    "        layer_types.append(type(sub_module))\n",
    "    layer_execution_order[name] = layer_types\n",
    "\n",
    "# Register hooks on each BasicBlock to capture layer order in forward pass\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, BasicBlock):\n",
    "        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n",
    "\n",
    "# Run the model forward pass to trigger hooks\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Define the expected order of layer types for BasicBlock\n",
    "expected_order = [nn.Conv1d, nn.BatchNorm1d, nn.ReLU]  \n",
    "\n",
    "# Check if each BasicBlock followed the expected order\n",
    "for name, order in layer_execution_order.items():\n",
    "    # Modify expected_order based on the chosen activation in model\n",
    "    current_expected_order = expected_order[:]\n",
    "    activation_fn_type = type(model.Hidden_1.activation_fn)  # Get the actual activation type\n",
    "    current_expected_order[-1] = activation_fn_type\n",
    "    \n",
    "    if order != current_expected_order:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(\n",
    "            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n",
    "            f\"but got {[cls.__name__ for cls in order]}.\"\n",
    "        )\n",
    "\n",
    "# Check output shape and range for LogSoftmax\n",
    "expected_shape = (1, 2)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "# Final success message if all tests pass\n",
    "if all_tests_successful:\n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348a88a-7021-436f-bc2a-0d8f64ac57d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ced37aa488680b86cd567830b9c8914",
     "grade": false,
     "grade_id": "cell-875fb13f0481da1b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Now, run the cell below to train and validate the model with the \"ReLU\" non-linearity, using Adam optimizer with lr = 0.0001.\n",
    "\n",
    "Repeat the training several times and compare the results with the model without the normalization layers. . Do you observe any changed with the patterns of instability or overfitting? \n",
    "\n",
    "Remember to save the model as **'normalized_model.pth'** and submit it to Moodle along with your other files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fe4abf-fa36-4db4-a4e3-34745fae4506",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"ReLU\").to(device)\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'normalized_model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99784f1-fa82-4745-bcc5-2a1b15263c75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebcc7d34a3a026996c7977d08efcc0de",
     "grade": true,
     "grade_id": "cell-07ab3b8da9f865fb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a2969-aed5-4a16-a5de-f8b1b6cbe62e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f04f0c0494ac7a2a03a139da0349204c",
     "grade": false,
     "grade_id": "cell-5fd7cc34830fc755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 4.2: Experiment with dropout layers\n",
    "\n",
    "Keep the normalization layers and add a dropout layer as the last layer of the convolutional block. \n",
    "\n",
    "Train the model, monitor the behavior of the training and validation curves, and observe how the dropout layers affects the validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03e716",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6eacbcd15e799d0f52c486613006516",
     "grade": false,
     "grade_id": "cell-f1f34b195b32a029",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    # Define a convolutional block based on the base model:\n",
    "    # - Add a batch normalization layer after the convolution layer and before the non-linearity function.\n",
    "    # - Add a dropout layer with p=0.5 at the end of the block (after the non-linearity function).\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "        \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 128\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create the forth hidden layer using BasicBlock\n",
    "        #    - Input channels: 128 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 5. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 6. Create a flattening layer to flatten the output for the final layer\n",
    "        # 7. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Pass the output to the forth hidden layer\n",
    "        # 5. Apply global average pooling to reduce dimensions\n",
    "        # 6. Flatten the pooled output\n",
    "        # 7. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf1f20-a5c4-4292-a15d-837c297c812c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52d60aa7a88a27de16782ab6b749b059",
     "grade": false,
     "grade_id": "cell-c0ce1bae2a506413",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb522759-5853-487f-bc4a-9af0b02f7ae8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f545476077c7f3fef59e641940a0ca7",
     "grade": true,
     "grade_id": "cell-095a0d6809e442f1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"ReLU\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "\n",
    "# Dictionary to hold the execution order of each BasicBlock's layers\n",
    "layer_execution_order = {}\n",
    "\n",
    "# Function to capture forward pass order of layers within each BasicBlock\n",
    "def track_execution_order(module, input, output, name):\n",
    "    layer_types = []\n",
    "    for sub_module in module.children():  # Iterate through layers within BasicBlock\n",
    "        layer_types.append(type(sub_module))\n",
    "    layer_execution_order[name] = layer_types\n",
    "\n",
    "# Register hooks on each BasicBlock to capture layer order in forward pass\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, BasicBlock):\n",
    "        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n",
    "\n",
    "# Run the model forward pass to trigger hooks\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Define the expected order of layer types for BasicBlock with Dropout\n",
    "expected_order = [nn.Conv1d, nn.BatchNorm1d, nn.ReLU, nn.Dropout]  \n",
    "\n",
    "# Check if each BasicBlock followed the expected order\n",
    "for name, order in layer_execution_order.items():\n",
    "    # Modify expected_order based on the chosen activation in model\n",
    "    current_expected_order = expected_order[:]\n",
    "    activation_fn_type = type(model.Hidden_1.activation_fn)  # Get the actual activation type\n",
    "    current_expected_order[2] = activation_fn_type  # Ensure activation function is dynamically set\n",
    "\n",
    "    if order != current_expected_order:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(\n",
    "            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n",
    "            f\"but got {[cls.__name__ for cls in order]}.\"\n",
    "        )\n",
    "\n",
    "# Check output shape and range for LogSoftmax\n",
    "expected_shape = (1, 2)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "# Final success message if all tests pass\n",
    "if all_tests_successful:\n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ab9b4-3f42-488a-9785-2788597edc7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e909ebf9c97585bee2d948b465fb402b",
     "grade": false,
     "grade_id": "cell-fb8b2ac41ed2f5fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Run the cell below to apply training and validation where the model is called with \"ReLU\" non-linearity. \n",
    "\n",
    "Repeat the training several times and compare the results with the model without the dropout layers. Do you observe any changed with the patterns of instability or overfitting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e0356",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"ReLU\").to(device)\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22077d-b53f-4d88-8c03-c61466c5900b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d61c017ecb67751dc5aa3bd384eb2e6f",
     "grade": false,
     "grade_id": "cell-6ad2a9685a406562",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 4.3: Experiment with efficient model architecture through pooling layers\n",
    "\n",
    "As the final step, try to increase the model efficiency through a wiser design of the model architecture. In a deep learning model, while using a stack of convolutional blocks, it is common practice to use pooling layers between convolutional layers to decrease the dimension of the data and the resolution of deeper layers. This helps make the model lighter by reducing the number of trainable parameters and, at the same time, increases the model's performance by helping it focus on different feature types at different layers. For example, in the case of audio processing, the shallower layers can focus on finding variations in short time windows, while deeper layers can focus on detecting longer variations.\n",
    "\n",
    "In this task, you are asked to follow the same logic and modify the model architecture using pooling layers.\n",
    "\n",
    "Save the trained model as **'pooled_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "In the template prepared for the model architecture below, fill in the blanks as instructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df61bbcd",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "287412d9af08a65094b3c40dc3c23d70",
     "grade": false,
     "grade_id": "cell-6395e9188699b28f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    # Define a convolutional block based on the base model:\n",
    "    # - Add a batch normalization layer after the convolution layer and before the non-linearity function.\n",
    "    # - Add a dropout layer with p=0.5 at the end of the block (after the non-linearity function). \n",
    "    def __init__(self, in_channels, out_channels, nonlin, kernel_size, stride ):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()        \n",
    "         # your code here for initializing layers\n",
    "        # 1. First hidden layer (BasicBlock)\n",
    "        #    - Input channels: 1 (initial input channel for audio or speech data)\n",
    "        #    - Output channels: 8\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        #    - Kernel size: 22, Stride: 2\n",
    "        # 2. Pooling layer after the first hidden layer\n",
    "        #    - Type: MaxPool1d\n",
    "        #    - Kernel size: 10 (to downsample the feature map)\n",
    "        #    - Stride: 2\n",
    "        # 3. Second hidden layer (BasicBlock)\n",
    "        #    - Input channels: 8\n",
    "        #    - Output channels: 16\n",
    "        #    - Activation function: specified by 'nonlin'     \n",
    "        #    - Kernel size: 22, Stride: 2\n",
    "        # 4. Pooling layer after the second hidden layer\n",
    "        #    - Type: MaxPool1d\n",
    "        #    - Kernel size: 10\n",
    "        #    - Stride: 2\n",
    "        # 5. Third hidden layer (BasicBlock)\n",
    "        #    - Input channels: 16\n",
    "        #    - Output channels: 32\n",
    "        #    - Activation function: specified by 'nonlin'        \n",
    "        #    - Kernel size: 22, Stride: 2\n",
    "        # 6. Global average pooling to reduce the output to a single value per channel     \n",
    "        # 7. Flatten layer to prepare for fully connected output\n",
    "        # 8. Fully connected layer to produce the final output with 'out_channels' classes\n",
    "        #    (assuming 'out_channels' is set to the number of output classes) \n",
    "        # 9. Final output activation function (LogSoftmax) to get class probabilities in log space\n",
    "        \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for forward pass through each layer     \n",
    "        # Pass through the first hidden block and pooling layer     \n",
    "        # Pass through the second hidden block and pooling layer\n",
    "        # Pass through the third hidden block\n",
    "        # Apply global average pooling to reduce each channel to a single value\n",
    "        # Flatten the output for the fully connected layer\n",
    "        # Pass through the fully connected layer\n",
    "        # Apply the output activation to produce log-probabilities  \n",
    "        # Apply the output activation to produce log-probabilities     \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410cdec-3969-4a63-b643-3af789855b84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b582dc5763098cab844ef02d5baaf51",
     "grade": false,
     "grade_id": "cell-91893612ea423f3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717128ee-c087-4596-bd88-c9cfb1dc8086",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "168887d8171b01ee55bdbee13cbfb726",
     "grade": true,
     "grade_id": "cell-2684368b24dce25c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"ReLU\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "expected_conv_count = 3  # Number of Conv1d layers expected\n",
    "if conv1d_count != expected_conv_count:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected {expected_conv_count} Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "# Check expected output channels\n",
    "expected_channels = [8, 16, 32]  # Expected output channels for the three Conv1d layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "\n",
    "# Check the output shape\n",
    "expected_shape = (1, 2)  # Expected output shape\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Check the number of trainable parameters\n",
    "def get_num_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 14490  # Expected number of trainable parameters\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "\n",
    "# Check the output range for LogSoftmax (should be <= 0)\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e6963-59d4-4361-98b9-cae532592d50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3053a964e1aab6361a553d9fb21c8b2",
     "grade": false,
     "grade_id": "cell-83249c188c363edd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Now, run the cell below to apply training and validation to your model. Remember to save the trained model as **'pooled_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "Compare the results with those from previously tested models. Observe how the hierarchical design of the convolutional blocks influences the model's performance on the training and validation splits. Additionally, take note of how this design impacts the number of trainable parameters and the overall training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bb929-3358-4c41-b18e-08d59bd76843",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfe81b3d3b6b418d468e812ed34958ac",
     "grade": false,
     "grade_id": "cell-1face21712218aae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"ReLU\").to(device)\n",
    "get_num_trainable_parameters(model)\n",
    "\n",
    "# optim\n",
    "learning_rate = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'pooled_model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40c5289-ad0a-4355-981b-b63f0f3bc2ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "302d386ae1437a9495761735de12c719",
     "grade": true,
     "grade_id": "cell-e77eb518e15f1d74",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests for checking the performance of the trained model\n",
    "all_tests_successful = True\n",
    "if not skip_training:\n",
    "    try:  \n",
    "        # Test 1: Ensure training accuracy is within the correct range\n",
    "        max_tacc = max(history['train_accuracy'])\n",
    "        if not (0.5 <= max_tacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.7, 1].\")\n",
    "            \n",
    "        # Test 2: Ensure accuracy is within the correct range\n",
    "        max_vacc = max(history['val_accuracy'])\n",
    "        if not (0.5 <= max_vacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.7, 1].\")\n",
    "    \n",
    "        if all_tests_successful:\n",
    "            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n",
    "    \n",
    "    except AssertionError as e:\n",
    "        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974b53e-c4c8-4109-84f5-cbaa22d66785",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6410c4d5badd38ce468b2954b6b233df",
     "grade": true,
     "grade_id": "cell-a0ed594dbd492924",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
