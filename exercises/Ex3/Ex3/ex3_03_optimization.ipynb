{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c65b2c4e",
   "metadata": {},
   "source": [
    "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563db0a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6d59fcf523de774eca0c4bd7ab8423e",
     "grade": false,
     "grade_id": "cell-910d80dbd222bcaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "## Task 3: Optimization Practices (5 Points)\n",
    "\n",
    "In this task, you will practice optimization techniques by experimenting with different optimizers, learning rates, and batch sizes. The main objectives include observing the performance and computational costs related to training time. Additionally, pay attention to the training curves: in deep learning, smoother training and validation curves are generally desirable, as they often indicate stable learning and consistent generalization.\n",
    "\n",
    "You can achieve smoother training curves by adjusting the learning rate and increasing the batch size. Another important factor to consider is batch variability through data shuffling. While data shuffling can enhance training, very high learning rates, combined with shuffling, can negatively impact the smoothness and stability of the training process, potentially leading to unstable learning.\n",
    "\n",
    "By completing this task, you will gain insights into how different optimization choices affect the training dynamics of your model.\n",
    "\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "\n",
    "**Task 3.1: Experiment with Adam optimizer with different learning rates** (1 point)\n",
    "\n",
    "    Goal: Increase the learning rate in each test to find the optimal learning rate.\n",
    "\n",
    "**Task 3.2: Experiment with Adam optimizer with different learning rates and shuffling data in each batch** (1 point)\n",
    "\n",
    "    Goal: Allow the data to be shuffled in each batch, increase the learning rate, and observe its effect.\n",
    "\n",
    "**Task 3.3: Experiment with Adam optimizer and different batch sizes** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of batch sizes on training stability and efficiency.\n",
    "\n",
    "**Task 3.4: Experiment with SGD optimizer with different learning rates and shuffling data in each batch** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of data variability and learning rate in the SGD optimizer.\n",
    "\n",
    "**Task 3.5: Experiment with SGD optimizer and batch normalization layers** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of batch normalization layers in the SGD optimizer.\n",
    "    \n",
    "\n",
    "### Deliverables from this task:\n",
    "\n",
    "* ex3_03_optimization.ipynb\n",
    "* 'lr_model.pth'\n",
    "* 'shuffle_model.pth'\n",
    "* 'bs_model.pth'\n",
    "* 'SGD_model.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8445f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False   # You can set it to True if you want to run inference on your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c688be9-b20b-4996-979e-ee2555da635b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c1ac106c142bbd64dabe53525b11c45",
     "grade": true,
     "grade_id": "cell-16ebf98cca481d32",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6feccd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# Set random seeds for all libraries\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1) \n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f32a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "883bbb47b37e7f80a3a0687a34aefb15",
     "grade": false,
     "grade_id": "cell-895bd361957c1108",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272855a-cca5-4a9e-b96b-84af270fae60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94a18ae73734f2f529dd312711a2e593",
     "grade": false,
     "grade_id": "cell-781c2fcb5766466d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Add the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573e466-40f0-41a8-adcc-7015cb657a41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"dataset_ex3\" # you can change the path if you want to store the dataset somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8a919c-72cb-4472-a101-90df507c5a06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c76e468a8231134e85d970a6032cf859",
     "grade": true,
     "grade_id": "cell-888628addb87c0ca",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403efef-8583-4c83-bcd8-efe439d40ab3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85faa59e1ac9689ab67019f60507ddc1",
     "grade": false,
     "grade_id": "cell-7429909f377b1aef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_file_list(path, validation_split):\n",
    "    \n",
    "    audio_class_names = ['music', 'speech']\n",
    "    data_directories = {'music': path + '/music_wav', \n",
    "                        'speech': path + '/speech_wav'}\n",
    "    \n",
    "    audio_files = {class_name: [] for class_name in audio_class_names}\n",
    "    for class_name in audio_class_names:\n",
    "        folder = data_directories[class_name]\n",
    "        filelist = os.listdir(folder)\n",
    "        for filename in filelist:\n",
    "            if filename.endswith('.wav'):\n",
    "                audio_files[class_name].append(os.path.join(folder, filename))\n",
    "       \n",
    "    np.random.seed(1)\n",
    "    dataset_split = {'train': [], 'val': []}\n",
    "    for class_id, class_name in enumerate(audio_class_names):\n",
    "        n_data = len(audio_files[class_name])\n",
    "        random_indices = np.random.permutation(n_data)\n",
    "        n_validation = int(validation_split * n_data)\n",
    "        val_indices = random_indices[:n_validation]\n",
    "        train_indices = random_indices[n_validation:]\n",
    "        dataset_split['train'] += [(audio_files[class_name][k], class_id) for k in train_indices] \n",
    "        dataset_split['val'] += [(audio_files[class_name][k], class_id) for k in val_indices] \n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ec9a3-ae6d-46ee-952b-4ebfe9f7672b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "722635170df78438e061235185a70864",
     "grade": false,
     "grade_id": "cell-1546acec0982c4ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MSDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, filelist, sample_sec=5., is_train=True):\n",
    "        self.filelist = filelist\n",
    "        self.time_duration = sample_sec\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        _, sf = librosa.load(filelist[0][0], sr = None)\n",
    "        self.sf = sf\n",
    "        self.n_features = int(self.time_duration * sf)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        audio_file, class_id = self.filelist[i]\n",
    "        x, sf = librosa.load(audio_file, sr = None)\n",
    "        k = 0\n",
    "            \n",
    "        x = torch.from_numpy(x[k:k+self.n_features]).reshape(1,-1)\n",
    "        \n",
    "        return x, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13718aa0-f3db-4d9e-9f29-d29b3afaaa1e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ab60ad37f18e81685ab1b2e9d26a25b",
     "grade": false,
     "grade_id": "cell-2ec30b2c12df9c78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Model Architecture: \n",
    "\n",
    "Fill in the blanks as instructed in the code to design the model architecture as in the base model.\n",
    "\n",
    "**Hint:** Replace the next cell with the base model from the task 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0ae94-46d6-460a-ad6f-464105ac614a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8594a9c284429f982d3fc967e97943d",
     "grade": false,
     "grade_id": "cell-d38e7a41ff51df4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, nonlin=\"Tanh\"):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv1d(in_channels=in_channels,\n",
    "                                    out_channels=out_channels,\n",
    "                                    kernel_size=11,\n",
    "                                    stride=5)\n",
    "        if nonlin == \"ELU\":\n",
    "            self.activation_fn = nn.ELU()\n",
    "        elif nonlin == \"ReLU\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif nonlin == \"Tanh\":\n",
    "            self.activation_fn = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 32\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Input channels: 32 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 5. Create a flattening layer to flatten the output for the final layer\n",
    "        # 6. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Apply global average pooling to reduce dimensions\n",
    "        # 5. Flatten the pooled output\n",
    "        # 6. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123f574-62c2-48ef-af6e-66dac5187107",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04d0e1f10274a60c8b3e9c31ab92b79f",
     "grade": false,
     "grade_id": "cell-bd6b8ef63cff57f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Training and validation loops: \n",
    "\n",
    "Fill in the blanks as instructed in the code.\n",
    "\n",
    "**Hint:** Replace the next cell with the training and validation loops from your solution to Task 1 in the ex3_01_base_model.ipynb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b731df9-5f30-4168-a749-fe8607ad3856",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87930d357b022c8dac2ca500baad0524",
     "grade": false,
     "grade_id": "cell-489f20e61072497e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optim, model, loss_fn, dl_train, dl_val, hist=None):\n",
    "    np.random.seed(1)\n",
    "    if hist is not None:\n",
    "        pass\n",
    "    else:\n",
    "        hist = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    t_initial = time.time()\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        start = time.time()\n",
    "        train_loss, train_accuracy = 0., 0.\n",
    "        num_samples = 0\n",
    "        \n",
    "        for input_batch, target_batch in dl_train: \n",
    "            # your code here for minibatch training\n",
    "            # 1. call batch data and labels and set them to the correct device\n",
    "            # 2. make the prediction on the data\n",
    "            # 3. calculate loss\n",
    "            # 4. set optimizer to zero grad\n",
    "            # 5. do backward pass\n",
    "            # 6. move the optimizer one step forward\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # accumulate correct prediction\n",
    "            train_accuracy += (torch.argmax(predictions.detach(), dim=1) == target_batch).sum().item() # number of correct predictions\n",
    "            train_loss += loss_train.item() * input_batch.shape[0]\n",
    "            num_samples += input_batch.shape[0]\n",
    "        \n",
    "        train_loss /= num_samples\n",
    "        train_accuracy /= num_samples       \n",
    "        val_loss, val_accuracy = validation_loop(model, loss_fn, dl_val)\n",
    "        \n",
    "        end = time.time()\n",
    "        epoch_time = round(end - start, 2)\n",
    "        if epoch <= 5 or epoch % 10 == 0 or epoch == n_epochs:\n",
    "             print(f'Epoch {epoch}, train_loss {train_loss:.2f}, train_accuracy: {train_accuracy:.4f}, '\n",
    "                   f'val_loss {val_loss:.2f}, val_accuracy: {val_accuracy:.4f}, time = {epoch_time}')\n",
    "\n",
    "        # record for history return\n",
    "        hist['train_loss'].append(train_loss)\n",
    "        hist['val_loss'].append(val_loss) \n",
    "        hist['train_accuracy'].append(train_accuracy)\n",
    "        hist['val_accuracy'].append(val_accuracy)\n",
    "        \n",
    "    t_final = time.time()\n",
    "    t_total = round(t_final - t_initial, 2)\n",
    "    minutes = int(t_total // 60)\n",
    "    seconds = int(t_total % 60)\n",
    "    print(f'Finished training_loop() within {minutes} minutes and {seconds} seconds')\n",
    "    return hist\n",
    "\n",
    "def validation_loop(model, loss_fn, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_accuracy, num_samples = 0., 0., 0.\n",
    "        \n",
    "        for input_batch, target_batch in dataloader:\n",
    "            # your code here for minibatch validation\n",
    "            # 1. set input_batch, target_batch to correct device\n",
    "            # 2. make the prediction on input_batch\n",
    "            # 3. calculate loss and add it to previous loss\n",
    "            # 4. obtain predicted class labels from predictions (hint: use torch.argmax)\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            ###\n",
    "            \n",
    "            total_accuracy += (predicted_classes == target_batch).sum().item()\n",
    "            num_samples += len(target_batch)\n",
    "    \n",
    "    average_loss = total_loss / num_samples\n",
    "    average_accuracy = total_accuracy / num_samples\n",
    "    \n",
    "    return average_loss, average_accuracy\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].plot(history['train_loss'], label='Train')\n",
    "    axes[0].plot(history['val_loss'], label='Validation')\n",
    "    axes[0].legend()\n",
    "\n",
    "    max_val_accuracy = max(history['val_accuracy'])\n",
    "    axes[1].set_title(f'Accuracy (Best: {max_val_accuracy:.2f})')\n",
    "    axes[1].plot(history['train_accuracy'], label='Train')\n",
    "    axes[1].plot(history['val_accuracy'], label='Validation')\n",
    "    axes[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f1330-1151-45de-a3c5-96fee4f70d7e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96fac19acbcb98f55f2922c6708c52ca",
     "grade": false,
     "grade_id": "cell-4bcbeeb52e414dac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 3.1: Experiment with Adam optimizer with different learning rates\n",
    "\n",
    "Use the settings applied in Task 1 for the base model:\n",
    "\n",
    "- `batch_size = 8`\n",
    "- `optimizer = \"Adam\"`\n",
    "\n",
    "Gradually increase the learning rate from 0.0001 (used in the base model) to observe its effect and find the optimal learning rate for this setting. In particular, test the following learning rates:\n",
    "\n",
    "- `lr = [0.0001, 0.001, 0.01, 0.1]`\n",
    "\n",
    "Save the best-performing model as **'lr_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "**Goal**: Identify how different learning rates impact model performance, stability, and training efficiency for the current setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a4c31c-f517-42bc-81cf-6781b54a0ec9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ea8c3a6bc421958056b3637406bbe8b",
     "grade": false,
     "grade_id": "cell-38d287f21bf5e60a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7c86d-1714-43f5-a43b-73cbc74772fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'lr_model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a48a3-ea0a-43dc-ac36-7d57f35d4229",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25e9167aa599263f7e77c92b114cf29d",
     "grade": true,
     "grade_id": "cell-a7d71b491709b69b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703756d-a15a-4940-aa3c-809c6ef10fb3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3298b55742d175aacc90e43ff9350373",
     "grade": false,
     "grade_id": "cell-0618474b78395671",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 3.2: Experiment with Adam optimizer with different learning rates and shuffling data in each batch \n",
    "\n",
    "Repeat the experiments from Task 3.1, this time with data shuffling enabled to increase batch variability (by setting `shuffle=True`). Test the following learning rates:\n",
    "\n",
    "- `lr = [0.0001, 0.001, 0.01, 0.1]`\n",
    "\n",
    "Save the best-performing model as **'shuffle_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "**Goal**: Identify how different learning rates impact model performance and stability in the presence of data variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e44df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "787e025c91aea42389a8aefb16caea0c",
     "grade": false,
     "grade_id": "cell-0904e29730bd7c51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True) for tv in ['train', 'val']}\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f10a59-a020-431c-b823-7ad6c006d449",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'shuffle_model.pth')\n",
    "    plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13baa262-437c-4425-acd1-2dbac914a144",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3cf7ece36ad12263866a125544f1680",
     "grade": true,
     "grade_id": "cell-b5bcaaf91493f1a5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33235ef3-d76d-46a4-bae4-62ed958d33c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34c2a5abef7177b9fe6738a3b9e7d23e",
     "grade": false,
     "grade_id": "cell-1cd3b7cae2bd4c92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 3.3: Experiment with Adam optimizer and different batch sizes\n",
    "\n",
    "Now, set the optimizer to Adam with `lr = 0.001` and disable data shuffling (`shuffle=False`). Try different batch sizes as specified below:\n",
    "\n",
    "- `bs = [4, 8, 16, 32]`\n",
    "\n",
    "Save the best-performing model as **'bs_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "**Optional:** You may repeat the experiments above with data shuffling enabled (`shuffle=True`) to observe the impact of batch size when data variability is increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb4395-f970-4e22-bf48-66befb26bbc8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb9b91a-cbe8-4451-9b9e-6a7a9937514d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fb03cb91349e2cf16dda9d05f93cd91",
     "grade": false,
     "grade_id": "cell-2950f97da2be9a32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'bs_model.pth')\n",
    "    plot_history(history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7876b457-43a8-4657-b73e-2ee0e4970fdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e8c72e0c9b88156988f908c3808093e",
     "grade": true,
     "grade_id": "cell-51faccba6463e6731",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e93e2cb-aec3-4477-8580-4549f85eb1c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "648a86753206431b64e3371995a5c1d8",
     "grade": false,
     "grade_id": "cell-70d052bd43eb1adb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 3.4: Experiment with SGD optimizer with different learning rates and shuffling data in each batch\n",
    "\n",
    "Repeat the experiments from Task 3.2, keeping data shuffling enabled, and change the optimizer to SGD. Test the following learning rates:\n",
    "\n",
    "- `lr = [0.0001, 0.001, 0.01, 0.1]`\n",
    "\n",
    "Save the best-performing model as **'SGD_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "**Goal**: Observe the different behaviors of Adam and SGD optimizers, and examine how the choice of optimizer impacts the selection and effect of learning rate on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28ecfb-c569-4286-9a54-adb490a103a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2efa005fdf2dd03bd699cb1d23c2d289",
     "grade": false,
     "grade_id": "cell-6d78644ccccb4fdc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 8\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True) for tv in ['train', 'val']}\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94258a3-2702-455b-948f-426f200b7b79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'SGD_model.pth')\n",
    "    plot_history(history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe15340-f0dd-4dca-9fea-4eb5e2a8d8ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e557e7ffd6b9c2dd6521b3ffe6caaea9",
     "grade": true,
     "grade_id": "cell-df6dd53a5d20fc968",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b013b0-1e45-4b8d-988b-e245ce6539cc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de43a4393d5cf634c4477bb2bf91a9c7",
     "grade": false,
     "grade_id": "cell-d2394f03baaeff88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 3.5: Experiment with SGD optimizer and normalization layers\n",
    "\n",
    "Next, test the effect of batch normalization layers on the SGD optimizer using the following setup for training:\n",
    "\n",
    "- Batch data shuffling disabled (`shuffle=False`)\n",
    "- `lr = 0.01`\n",
    "- `bs = 16`\n",
    "\n",
    "Use the base model, then add a batch normalization layer within each convolutional block. Place this layer directly after each convolution layer and before applying the non-linearity function, as indicated in the code.\n",
    "\n",
    "**useful link**: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d6ada",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97f4a56472ea5cd85beca9c265a26e63",
     "grade": false,
     "grade_id": "cell-ae8830d6b50743a1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    # your code here the convolutional block. Use the base model, \n",
    "    # add one batch normalization layer after convolution layer and before non-linearity function   \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nonlin =\"Tanh\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        # your code here for initializing layers\n",
    "        # 1. Create the first hidden layer using BasicBlock\n",
    "        #    - Input channels: 1 \n",
    "        #    - Output channels: 32\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 2. Create the second hidden layer using BasicBlock\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 3. Create the third hidden layer using BasicBlock\n",
    "        #    - Input channels: 32 (from the output of the second layer)\n",
    "        #    - Output channels: 2 (for the final output classes)\n",
    "        #    - Activation function: specified by 'nonlin'\n",
    "        # 4. Create a global average pooling layer to reduce the spatial dimensions\n",
    "        # 5. Create a flattening layer to flatten the output for the final layer\n",
    "        # 6. Set the output activation function for classification\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "       \n",
    "    def forward(self, x):\n",
    "        # your code here for calling layers\n",
    "        # 1. Pass the input through the first hidden layer\n",
    "        # 2. Pass the output to the second hidden layer\n",
    "        # 3. Pass the output to the third hidden layer\n",
    "        # 4. Apply global average pooling to reduce dimensions\n",
    "        # 5. Flatten the pooled output\n",
    "        # 6. Apply the output activation function to get the final predictions\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c6945-5ca4-4729-88df-1da10f700864",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d832433956e3de250e2dfd4bcf1e73b7",
     "grade": false,
     "grade_id": "cell-c0594868fee75ccf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b7210-6123-4daa-916a-e769191a691f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "868918b333d79606b39f3cbee95e19f5",
     "grade": true,
     "grade_id": "cell-f5aa944ceec424c3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(\"Tanh\")\n",
    "dummy_input = torch.randn(1, 1, 22000)\n",
    "\n",
    "# Dictionary to hold the execution order of each BasicBlock's layers\n",
    "layer_execution_order = {}\n",
    "# Function to capture forward pass order of layers within each BasicBlock\n",
    "def track_execution_order(module, input, output, name):\n",
    "    layer_types = []\n",
    "    for sub_module in module.children():  # Iterate through layers within BasicBlock\n",
    "        layer_types.append(type(sub_module))\n",
    "    layer_execution_order[name] = layer_types\n",
    "\n",
    "# Register hooks on each BasicBlock to capture layer order in forward pass\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, BasicBlock):\n",
    "        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n",
    "\n",
    "# Run the model forward pass to trigger hooks\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Define the expected order of layer types for BasicBlock\n",
    "expected_order = [nn.Conv1d, nn.BatchNorm1d, nn.Tanh]  \n",
    "\n",
    "# Check if each BasicBlock followed the expected order\n",
    "for name, order in layer_execution_order.items():\n",
    "    # Modify expected_order based on the chosen activation in model\n",
    "    current_expected_order = expected_order[:]\n",
    "    activation_fn_type = type(model.Hidden_1.activation_fn)  # Get the actual activation type\n",
    "    current_expected_order[-1] = activation_fn_type\n",
    "    \n",
    "    if order != current_expected_order:\n",
    "        all_tests_successful = False\n",
    "        raise AssertionError(\n",
    "            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n",
    "            f\"but got {[cls.__name__ for cls in order]}.\"\n",
    "        )\n",
    "\n",
    "# Check output shape and range for LogSoftmax\n",
    "expected_shape = (1, 2)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "if not torch.all(dummy_output <= 0):\n",
    "    all_tests_successful = False\n",
    "    raise AssertionError(\"The output values are not within the expected range (-∞, 0]. LogSoftmax might be missing.\")\n",
    "\n",
    "# Final success message if all tests pass\n",
    "if all_tests_successful:\n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f4ae3-6197-400a-a4aa-496ca90c127f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddbc433e067c0c31b049e9cff34507b7",
     "grade": false,
     "grade_id": "cell-cc0e56684bb89065",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train and Validate\n",
    "\n",
    "Now, run the cell below to apply training and validation for the model with the batch normalization layer using the SGD optimizer with a learning rate of 0.01.\n",
    "\n",
    "Compare the performance and stability of the training and validation curves with the settings from Task 3.3 to observe the effect of the batch normalization layer on the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fe111-0067-463d-9103-2d3a310e0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "np.random.seed(1)  \n",
    "valsplit = 0.30\n",
    "file_list =  prepare_file_list(path, valsplit)  \n",
    "sample_sec = 2\n",
    "batch_size = 16\n",
    "data_loader = {tv: torch.utils.data.DataLoader(MSDataset(file_list[tv], sample_sec=sample_sec, is_train=tv=='train'),\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=False) for tv in ['train', 'val']}\n",
    "\n",
    "# model\n",
    "model = MyModel(\"Tanh\").to(device)\n",
    "\n",
    "# optim\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = 0.9)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# history\n",
    "history = None\n",
    "if not skip_training:\n",
    "    history = training_loop(300, optimizer, model, criterion, data_loader['train'], data_loader['val'], history)\n",
    "    torch.save(model.state_dict(), 'model.pth')\n",
    "    plot_history(history)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
